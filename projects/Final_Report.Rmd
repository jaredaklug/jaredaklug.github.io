---
title: "P8160 Group Project 1: Variable Selection Simulation Study"
author: "Wenbo Fei, Jared Klug, Jeffrey Liang, and Madison Stoms"
output: html_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(knitr)

knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  echo = T,
  warning = F
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d



sce_1 = read_csv("./result/sce_1.csv")
sce_2 = read_csv("./result/sce_2.csv")
sce_3 = read_csv("./result/sce_3.csv")
sce_4 = read_csv("./result/sce_4.csv")
```

```{r task, echo = F, include=F}
"you have two tasks in your simulations. The first one is to evaluate how well the two methods (Forward AIC and LASSO) select the correct models. i.e., how well they identify strong and weak signals and exclude the covariates that have no associations. how their performances vary with sample size, the number of covariates, and between-covariate correlations.) The second is to assess the impact of excluding some weak signals on estimating the coefficients with strong signals. You may view the strong-signal as the treatment effect, which is of the primary interest, and view the weak signals as the controlling covariates. The second part's simulation demonstrates the importance of identifying the weak signals and keeping them in the model. Combining both simulations, you can make practical recommendations on which variable selection method is recommended under which scenario. "
```



# 1 Introduction

In the era of big data the primary goal of the statistician becomes more complex then ever before: to extract the signal from the noise. In the setting of model building, this task reduces to selecting an optimal model which balances fit and complexity. A popular class of methods to accomplish this are variable selection tools. The general idea of variable selection is to select a model which minimizes the error while controlling for the number of predictors. This paper focuses on two common variable selection methods: Step-wise forward selection and automated LASSO regression.  

Step-wise forward selection begins with an intercept only model and then sequentially adds one variable at a time to the model. At each step, the model is tested to see whether the added variable improves the AIC which is given by: 
$$AIC = n\ln(\sum_{i=1}^n (y_i - \widehat{y}_i)^2/n) + 2p$$, where $\widehat{y}_i$ are the fitted values from the model, and $p$ is the dimension of the model.
The iteration stops when adding predictors no longer improves the AIC. Automated LASSO regression is performed by minimizing a penalized loss function given by:   
$$\min_\beta \frac{1}{2n} \sum_{i=1}^n (y_i - x_i \beta )^2 + \lambda \lVert \sum_{k=1}^p|\beta_k|$$
where $\lambda$ is a tuning parameter.
This penalty term forces the optimization process to rely on both metrics of error and parsimony.  

Though straightforward in theory, these methods tend to fall short in application, particularly when the true model contains weak predictors. Our objective is to illustrate how well LASSO and step-wise regression perform in identifying weak and strong predictors as well as the effect that failing to select weak predictors has on parameter estimates. To do this, we will perform a simulation study in which we generate models to evaluate the methods' ability to identify the truth under various settings. 


# 2 Methods

## 2.1 Data Generation

The predictors $\boldsymbol{X_{n\times p}}$ are generated from the standard normal distribution using `rnorm()`. Moreover, to generate correlated predictors, we use `mgcv::rmvn()` with correlations randomly generated between {$\pm0.2, ..., \pm0.9$} if not pre-specified. For simplicity, we assume each weak but correlated predictor is only associated with one strong predictor. Each strong predictor can correlate with zero or multiple weak predictors depending on specification.

The response $\boldsymbol{Y_{n\times1}}$ is defined by
$$\boldsymbol{Y}_{n*1} = \boldsymbol{X}_{n*p}*\boldsymbol{\beta}_{n*1} + \boldsymbol{\epsilon}_{n*1}$$

The parameter vector $\boldsymbol{\beta}$ is randomly generated from a uniform distribution. For strong predictors, $\beta_{strong}$ is generated from $U(5\times threshold,10\times threshold)$, where threshold = $c\sqrt{log (p) / n}$. The term c is pre-specified with default = 1. For weak predictors, $\beta_{weak}$ is generated from $U(\frac{threshold}{5},\frac{threshold}{2})$ and for null predictors, $\beta_{null} = 0$. The numbers of "strong", "weak but correlated", "weak and independent" and null predictors can be calculated from `(p, p.true. p.strong, ratio_weak_cor)` and vary across simulation settings. 

The error term $\boldsymbol{\epsilon}$ is generated from $N(\vec{0},\sigma\boldsymbol{I})$, $\sigma$ is pre-specified with default = 0.5.


## 2.2 Simulation Settings

\ In order to evaluate the selection methods under various settings, we constructed four simulation scenarios. The *first* scenario was constructed to answer the question: (1) How do the methods perform when the ratio of strong, weak but independent, and weak but correlated predictors is varied? To do this, we ran 100 simulations in which we fixed the true number of predictors to be 50 and varied the proportion of predictors that fell in each of the three categories specified above.  

The *second* scenario aimed to answer the question: (2) How do the presence of null predictors influence the variable selection methods. We ran 100 simulations in which we varied the number of null predictors included in the model from 50 to 750.  

The *third* scenario answers the question: (3) How does the sample size affect the ability of the methods to select the strong predictors. Fixing the number of predictors to be 100 (50 null predictors), we ran 10 simulations in which we varied the sample size from 300 to 1000.  

Finally, the *fourth* scenario was constructed to address the final question: (4) How does the correlation between the weak but correlated predictors affect variable selection? To do this, we fixed the sample size, number of predictors and ran 100 simulations varying the correlation from -0.8 t0 0.8.  

The table below summarizes the differences between the four settings. Running LASSO and stepwise selection in each of these cases allows us to identify the strengths and weaknesses of the methods in each of the scenarios.

```{r, echo = FALSE}
tibble(
  
  setting = c("Scenario 1", "Scenario 2", "Scenario 3", "Scenario 4"),
  N = c(100, 10, 100, 100),
  n = c(1000, 1000, "300:1000", 1000),
  p.true = c(50, 50, 50, 50), 
  p = c(50, "100:800", 100, 100),
  cor = c("random", "random", "random", "-0.8:0.8")
  
) %>% 
  kable(col.names = c("", "Simulations", "Sample Size", "True Predictors",
                      "Total Predictors", "Correlation"))
```



## 2.3 Performance Measures

In order to evaluate how well each method performs in identifying weak and strong predictors, the ratio of the number of detected true predictors to number of true predictors were recorded. As the number of strong and weak predictors vary for each simulation setting, the ratio is a stable measurement that can show how each model performed under the varying proportions of true predictors and predictor types. 

Also, the number of misclassified predictors in each model is recorded. Misclassified predictors are defined as the sum of the true predictors not selected and the null predictors selected. 

Finally, for each type of predictor, we evaluate the bias. The bias is calculated by computing the mean square of the difference between the estimated beta coefficients and the true coefficient value for each predictor $(\hat{\beta} - \beta)$. The bias is also used to identify if the models overestimate or underestimate the beta coefficients. 

# 3 Results

## Scenario 1

```{r tbl_scenario_1,echo=F}
sce_1 %>% 
  filter(model == "step") %>% 
  select(p.strong:weak.ind,misclassification,positive_bias,negative_bias) %>% 
  knitr::kable(caption = "Scenario 1: AIC performance")

sce_1 %>% 
  filter(model == "lasso") %>% 
  select(p.strong:weak.ind,misclassification,positive_bias,negative_bias) %>% 
  knitr::kable(caption = "Scenario 1: LASSO performance")
```

![Estimation MSE of predictors by type and model](GP1_try0214_files/figure-html/number_of_predictor_in_final_model-1.png)

![MSE vs Weak&Correalted to Strong Predictors Ratio](./GP1_try0214_files/figure-html/MSE_to__weak_correlated_to_strong-1.png)

\ From Fig.1, the MSE of strong predictors is higher for LASSO method compared to AIC. The estimation for weak and correlated predictors has close to no bias in the case of a fewer number (less than 10) of correlated weak predictors and increases as the number of correlated weak predictors increases for both models. The MSE of correlated weak predictors is higher in AIC compared to LASSO method as the number of correlated weak predictors increases. The MSE of weak and independent predictors increases consistently as the number of weak and independent predictors increase. The MSE of LASSO is lower than AIC method until the number of weak and independent predictors passes 45 in our scenario 1 settings. From Table.2 we can see that AIC equally over or underestimates the parameters, while LASSO mostly underestimates. From Fig.2 one can observe that as the number of strong predictors increase and the ratio of weak and correlated to strong predictors decrease, the mean square bias of all predictors decreases in the LASSO method. For AIC method, the MSE behaves similarly to LASSO, but the MSE for strong predictor does not share the decreasing trend as LASSO.

\ The rate of strong predictors being detected is approximately 1, except for a few settings where the number of strong predictors is less than 10 in scenario 1 for LASSO method. The rate of weak and correlated predictors being detected is close to 0 at the fewer number of weak and correlated predictors in our scenario 1, and increase to 0.25 to 0.40 as the number of weak and correlated predictors increases for both AIC and LASSO method. The detection rate for weak and independent predictors is consistent at 0.5 for AIC method. For LASSO method, the detection rate of weak and independent predictors decrease from approximately 0.6 to 0.25 as the number of weak and independent predictors increases.

\ From Table.3 we can see that as the number of strong predictors increases, the overall misclassification of predictors decreases in both methods. Specifically, the AIC method performs better in terms of misclassification before the number of strong predictors surpass 14 in Scenario 1 settings. Fixing the number of strong predictors, it shows that the number of misclassifications decrease as the number of weak and correlated predictors decrease for both methods.


## Scenario 2

![Scenario 2: MSE vs number predictor](./GP1_try0214_files/figure-html/scenario_2_mse_vs_number_of_parameters-1.png)

\ From Fig.3 we can observe that the estimated MSE is lower and consistent for strong predictors in the absence of weak and correlated predictors, and increases with the number of weak and correlated predictors in both AIC and Lasso methods. The MSE of weak and independent predictors increases with the absence of weak and independent predictors in both models. The AIC method has lower estimated MSE compared to lasso method, but there is an upward trend as the number of predictors approach the sample size.

\ Fig.4 shows that each method consistently selected the strong predictors with a detection rate of 1 irregardless of the amount of null predictors. However, as the number of number of null predictors increases, the LASSO method will misclassify more true predictors as null predictors, and the AIC method performs better as the amount of null predictors increases. 

\ From Fig.5 In both methods, as the number of parameters increases, the total amount of misclassified predictors also increases. The ratio of weak correlated predictors to strong predictors does not seem to impact the amount of misclassificaiton. From this figure, we are able to see that LASSO does a better job of not misclassifying predictors compared to AIC method. 

\ Both plots reveal that although the forward stepwise method does a better job of selecting true predictors, it has a much higher amount of missclassified predictors in the final model. In can be extrapolated that there are more null predictors in our AIC model, as well as true predictors, while the LASSO models contain much less null predictors. 


![Scenario 2: Detection rate vs number of predictors](./GP1_try0214_files/figure-html/scenario_2_detection_vs_number_of_parameters-1.png)

![Scenario 2: Misclassification vs Number predictors](./GP1_try0214_files/figure-html/misclassify_scenario_2-1.png) 

\ From Fig.4 detection rate is consistent to 1 for strong predictors in both model regardless of the number of predictors. For weak and correlated data and weak independent predictors, AIC method has a higher detection rate compared to lasso method and shows a ascent trend as the number of predictors increases. In addition, from Fig.5 it shows that misclassification is relatively consistent in Lasso method, regardless of the number of predictors in the model. AIC on the other hand, has a increasing number of misclassification as the number of predictors in the model increases, not selecting the correct models.

## Scenario 3

![Rate of Detection vs. Sample Size](GP1_try0214_files/figure-html/detect_scenario3-1.png)

![Misclassification vs. Sample Size](GP1_try0214_files/figure-html/misclassify_scenario_3-1.png)


\ Scenario 3 allows us to analyze how varying sample sizes may impact the ability of each method to select true predictors. In our tests, LASSO and forward stepwise methods correctly selected all strong predictors consistently no matter the sample size. However, each method failed to select a majority of weak predictors (both weak-correlated and weak-independent predictors). The AIC method had consistent performance with a detection rate of ~.5 no matter the sample size. The LASSO method's detection rate has a consistent negative correlation with sample size. 

\ Fig.6 reveals that as sample size increases, the amount of misclassified predictors in each model vary. Both models tend to misclassify 1 to 3 more predictors when there are weak but correlated predictors present. The LASSO method has more stable performance as it tends to misclassify 30-31 predictors on average in the simulations. There is no discernible relationship between sample size and number of misclassified for the LASSO method. However, the forward stepwise method benefits greatly with a larger sample size. There is a notable decrease in the total amount of misclassified predictors in the AIC model as sample size increases.

\ While the AIC models had a consistent performance of identifying 50% of the true weak predictors, it is evident that the models contained more null predictors, and as sample size increased was able to discern more predictors as true null predictors.

## Scenario 4

```{r, echo = FALSE}
cor_lasso = sce_4 %>% 
  filter(model == "lasso") %>% 
    select(cor,weak.corr, misclassification, strong_detected, weak_corr_detected, weak_ind_detected, weak_detected) %>% 
  rename("Correlation" = cor,
         `Weak and correlated predictors` = weak.corr) %>% 
  rename("Number Predictors Misclassifed" = misclassification) %>% 
  rename("Strong Detected Rate" = strong_detected) %>% 
  rename("Weak-Correlated Detected Rate" = weak_corr_detected) %>% 
  rename("Weak-Independent Detected Rate" = weak_ind_detected) %>% 
  rename("Total Weak Detected Rate" = weak_detected)

cor_step = sce_4 %>% 
  filter(model == "step") %>% 
  select(cor,weak.corr, misclassification, strong_detected, weak_corr_detected, weak_ind_detected, weak_detected) %>% 
  rename("Correlation" = cor,
         `Weak and correlated predictors` = weak.corr) %>% 
  rename("Number Predictors Misclassifed" = misclassification) %>% 
  rename("Strong Detected Rate" = strong_detected) %>% 
  rename("Weak-Correlated Detected Rate" = weak_corr_detected) %>%
  rename("Weak-Independent Detected Rate" = weak_ind_detected) %>% 
  rename("Total Weak Detected Rate" = weak_detected)

cor_lasso %>% knitr::kable(caption = "Scenario 4: LASSO performance")
cor_step %>% knitr::kable(caption = "Scenario 4: AIC performance")
```

![Strong MSE vs Correlation](./GP1_try0214_files/figure-html/cor_vs_strong_mse-1.png)

\ Fig.8 shows that correlation impacts the estimation of strong predictors differently for AIC and Lasso method. It shows that themean square estimation bias of strong predictor increase as the absolute correlation between weak and strong predictors and the number of weak and correlated to strong predictor ratio increase in the AIC method. Lasso however, behaves rather differently, as the estimate MSE for strong predictor in higher ratio settings(>1) decrease as the correlation increasing from -0.8 to 0.8, and closed to unbiased as the correlation lose to 0.8. The estimation MSE of Lasso for ratio equals to 1 setting does not impact by the correlation. 
There's might be interaction between the absolute correlation and the ratio, as the slope of higher ratio changes much significantly than lower ratio. Also, negative correlation seems to have a stronger impact on the estimation of strong predictors than positive correlation in the higher correlation settings for Lasso method. In contrast, negative and positive correlation impacts the estimation equally in AIC method, and interestingly, the strong predictor's estimation MSE doesnt seems to be affect by the weak and correlated predictors between correlation in -0.2 to 0.2.

\ From Table.4 we see that the LASSO method was able to select the all strong predictors everytime. For selecting both weak-correlated and weak-independent predictors, the LASSO method seems to perform better with no between-covariate correlations. The same pattern is observed for total number of misclassified predictors. 

\ From Table.5 we see that the AIC method has very similar performance to the LASSO method, however this method performed better at selecting true weak predictors. Like the LASSO method, the forward stepwise method performed better with a closer to null between-covariate correlation. The AIC method had a lower amount of misclassified predictors with no between-covariate correlation, but had a larger amount of misclassified predictors at the higher magnitude correlations. This indicates that as the between-covariate correlation increased in the positive and negative direction, the stepwise method selected more null predictors.



# 4 Discussion

## Forward Stepwise
\ Forward stepwise method is the most common variable selection strategy for logistic regression which tests several combinations of potential predictor variables starting from a single variable model and testing each next predictor to improve the AIC. The goal of our simulation is to understand the limitations of this model in variable selection how missing "weak" predictors impact the parameter estimates. 

\ Simulation scenarios 2-4 help us identify how variable selection performance is affected by varying number of null predictors, varying of sample size, and varying between-covariate correlation respectively. AIC was able to identify strong signals 100% of the time no matter the simulation setting. However, its performance in selecting weak predictors is impacted by these varying parameters in each scenario. 

\ As the number of null predictors increase, forward stepwise method gradually performs better at identifying true weak predictors. AIC was consistently better at identifying weak-independent predictors than weak-correlated predictors. Although the rate of detection of true predictors is increased as predictor size increases, the amount of null predictors selected also increases. This is a logical expectation as by chance some null predictors may increase the AIC of a model when added, and the increasing amount of null predictors will increase the amount of null predictors selected. Having no weak-correlated variables does not have a discernible impact on the AIC method's parameter selection compared to the presence of weak-correlated variables. The MSE of AIC method increase as the number of predictors increase, although not strictly linear. This points to the fact that stepwise selection is not suitable for high dimensional settings.

\ As the sample size of the input increases, this method has consistent performance of identifying true weak predictors. Although increasing the sample sizes does not affect the forward stepwise method's ability to identify true weak predictors, increasing the sample sizes helps the method to differentiate null predictors from true predictors. Also, the AIC method performs better at selecting variables with no weak-correlated predictors than data that contains weak-correlated predictors. 

\ Varying the between-covariate correlation heavily impacts this method's ability to select true weak-correlated predictors but has consistent performance at identifying true weak-independent predictors. At the large negative and positive correlation rates, the AIC method had an extreme low rate of selecting true weak-correlated predictors. The lower the between-covariate correlation resulted in the best variable selection performance for this model. The between-covariate correlation also impact the estimation MSE of strong preidctors in the extreme cases, but AIC perform rather stable for estimation for strong predictors in the moderate number and correlation of weak predictors settings, which can be rare in high-dimensional settings. 

\ Overall, the AIC method's variable selection ability performs best with data that contains lower number of parameters, large sample size, and low between-covariate correlation. It also outperformed the LASSO model in detecting true weak-correlated and weak-independent predictors.

## LASSO
\ The LASSO (Least Absolute Shrinkage and Selection Operator) method, does regression analysis using a shrinkage parameter and performs variable selection by forcing some coefficients of variables to zero through a penalty.

\ Like the AIC method, LASSO was able to identify strong signals approximately 100% of the time no matter the simulation setting. However, its performance in selecting weak predictors is impacted by these varying parameters in each scenario. 

\ As the number of null predictors increase, the LASSO method's ability to identify weak predictors decrease. For both weak-correlated and weak-independent predictors, LASSO was able to identify them as true predictors at similar rates. The LASSO method was also able to distinguish which predictors are null predictors at a rate much better than the AIC method even as the number of null predictors increased. There is also no distinguishable difference in the amount of misclassified predictors for the LASSO method with or without the presence of weak-correlated predictors. The MSE increase as the number of predictors increases, as the Lasso penalizing the model by $\lambda*\sum^p_{i=1}{|\beta_i|}$. Increase of predictors need to underestimate $\beta$ to minimized penalization.

\ As the sample size increases, the LASSO method performed worse at detecting true weak predictors. LASSO also drives more null predictor coefficients to zero as sample size increases. There is no noticeable pattern on the amount of total misclassified predictors in the model with increasing sample size.

\ Much like the AIC method, LASSO performed best for model selection when there is a no between-covariate correlation. However, unlike the AIC method where it had a consistent rate of identifying weak-independent predictors, the LASSO method performed best when there is no between-covariate correlation and had a noticeable decreased rate of detection at large negative and positive between-covariate correlation. In contrast to AIC methods, higher number of highly positive correlated weak predictors in fact decreasing the estimation MSE of strong predictors. The result can be explain by the scale,$$\hat{\beta}_{lasso}=\frac{S(<x,y>,\gamma)}{<x,x>}$$ in the higher positive correlated weak predictors increase the variance of estimator, and thus have a lower MSE. Similarily, higher negative correlated weak predictors results in a lower variance of the estimator and thus higher MSE. It is also susceptible to higher correlation. the estimation of predictor in lasso is scale sensitive, as a result, correlation greatly affect the variance of model, and thus we saw significant changes in slope. 

\ Overall, the LASSO method performs best in small datasets where there is likely a large number of null predictors compared to true predictors. The LASSO method is great at identifying strong predictors, however through our results the LASSO method will often force weak-correlated and weak-independent predictor coefficient to zero.

# Conclusion

\ AIC method has a better performance of including weak correlated and independent predictors and have lower over estimation MSE of strong predictors. However, AIC is limited by its incapability of identifying null predictors, and data size sensitive. And thus might perform badly in model selection in a higher dimensional settings.

\ Lasso on the other hand, much more likely to drop weak predictors and have a higher estimation MSE. But it has better ability to identify null predictors regardless of data size. Therefore, Lasso may have a lower false positive rate in the higher dimensional setting.

